{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "# Ejemplos con Hive"}, {"cell_type": "markdown", "metadata": {}, "source": "## Descripci\u00f3n de las variables"}, {"cell_type": "markdown", "metadata": {}, "source": "El dataset, obtenido de <a target = \"_blank\" href=\"https://www.transtats.bts.gov/Fields.asp?Table_ID=236\">este link</a> est\u00e1 compuesto por las siguientes variables referidas siempre al a\u00f1o 2018:\n\n1. **Month** 1-4\n2. **DayofMonth** 1-31\n3. **DayOfWeek** 1 (Monday) - 7 (Sunday)\n4. **FlightDate** fecha del vuelo\n5. **Origin** c\u00f3digo IATA del aeropuerto de origen\n6. **OriginCity** ciudad donde est\u00e1 el aeropuerto de origen\n7. **Dest** c\u00f3digo IATA del aeropuerto de destino\n8. **DestCity** ciudad donde est\u00e1 el aeropuerto de destino  \n9. **DepTime** hora real de salida (local, hhmm)\n10. **DepDelay** retraso a la salida, en minutos\n11. **ArrTime** hora real de llegada (local, hhmm)\n12. **ArrDelay** retraso a la llegada, en minutos: se considera que un vuelo ha llegado \"on time\" si aterriz\u00f3 menos de 15 minutos m\u00e1s tarde de la hora prevista en el Computerized Reservations Systems (CRS).\n13. **Cancelled** si el vuelo fue cancelado (1 = s\u00ed, 0 = no)\n14. **CancellationCode** raz\u00f3n de cancelaci\u00f3n (A = aparato, B = tiempo atmosf\u00e9rico, C = NAS, D = seguridad)\n15. **Diverted** si el vuelo ha sido desviado (1 = s\u00ed, 0 = no)\n16. **ActualElapsedTime** tiempo real invertido en el vuelo\n17. **AirTime** en minutos\n18. **Distance** en millas\n19. **CarrierDelay** en minutos: El retraso del transportista est\u00e1 bajo el control del transportista a\u00e9reo. Ejemplos de sucesos que pueden determinar el retraso del transportista son: limpieza de la aeronave, da\u00f1o de la aeronave, espera de la llegada de los pasajeros o la tripulaci\u00f3n de conexi\u00f3n, equipaje, impacto de un p\u00e1jaro, carga de equipaje, servicio de comidas, computadora, equipo del transportista, problemas legales de la tripulaci\u00f3n (descanso del piloto o acompa\u00f1ante) , da\u00f1os por mercanc\u00edas peligrosas, inspecci\u00f3n de ingenier\u00eda, abastecimiento de combustible, pasajeros discapacitados, tripulaci\u00f3n retrasada, servicio de inodoros, mantenimiento, ventas excesivas, servicio de agua potable, denegaci\u00f3n de viaje a pasajeros en mal estado, proceso de embarque muy lento, equipaje de mano no v\u00e1lido, retrasos de peso y equilibrio.\n20. **WeatherDelay** en minutos: causado por condiciones atmosf\u00e9ricas extremas o peligrosas, previstas o que se han manifestado antes del despegue, durante el viaje, o a la llegada.\n21. **NASDelay** en minutos: retraso causado por el National Airspace System (NAS) por motivos como condiciones meteorol\u00f3gicas (perjudiciales pero no extremas), operaciones del aeropuerto, mucho tr\u00e1fico a\u00e9reo, problemas con los controladores a\u00e9reos, etc.\n22. **SecurityDelay** en minutos: causado por la evacuaci\u00f3n de una terminal, re-embarque de un avi\u00f3n debido a brechas en la seguridad, fallos en dispositivos del control de seguridad, colas demasiado largas en el control de seguridad, etc.\n23. **LateAircraftDelay** en minutos: debido al propio retraso del avi\u00f3n al llegar, problemas para conseguir aterrizar en un aeropuerto a una hora m\u00e1s tard\u00eda de la que estaba prevista."}, {"cell_type": "markdown", "metadata": {}, "source": "Leemos el fichero CSV utilizando el delimitador por defecto de Spark (\",\"). La primera l\u00ednea contiene encabezados (nombres de columnas) por lo que no es parte de los datos y debemos indicarlo con la opci\u00f3n header."}, {"cell_type": "markdown", "metadata": {}, "source": "## 1. Leemos los datos desde el CSV que hay en Google Cloud Storage"}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": "# Esto no hace nada: la lectura es lazy as\u00ed que no se lee en realidad hasta que ejecutemos una acci\u00f3n sobre flightsDF\n# Solamente se comprueba que exista el fichero en esa ruta, y se leen los nombres de columnas\nflightsDF = spark.read.option(\"header\", \"true\")\\\n                 .csv(\"gs://ucm__bucket/data/flights-jan-apr-2018.csv\")"}, {"cell_type": "markdown", "metadata": {}, "source": "## 2. Vamos a mostrar el contenido del metastore de Hive, que ahora mismo no tiene nada"}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+--------+---------+-----------+\n|database|tableName|isTemporary|\n+--------+---------+-----------+\n+--------+---------+-----------+\n\n"}], "source": "spark.sql(\"show tables\").show()"}, {"cell_type": "markdown", "metadata": {}, "source": "## 3. Creamos una vista temporal de un DF que tiene solo dos columnas"}, {"cell_type": "markdown", "metadata": {}, "source": "Esto solamente a\u00f1ade metadatos al metastore de Hive, que adem\u00e1s se borrar\u00e1n cuando cerremos el notebook. No guarda datos f\u00edsicos de la tabla en ning\u00fan lado, puesto que el DF est\u00e1 en memoria. O mejor dicho, el DF del que proviene esta vista temporal \"no est\u00e1 en ning\u00fan lado\", porque no hemos cacheado weatherDistanceDF as\u00ed que cualquier consulta SQL que hagamos sobre la tabla `weatherDistanceTable` provoca que se tenga que re-calcular el DF `weatherDistanceDF` sobre el cual est\u00e1 creada dicha tabla temporal."}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [], "source": "weatherDistanceDF = flightsDF.select(\"WeatherDelay\", \"Distance\")\nweatherDistanceDF.createOrReplaceTempView(\"weatherDistanceTable\")"}, {"cell_type": "markdown", "metadata": {}, "source": "Ahora vemos que la tabla se ha creado como tabla temporal"}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+--------+--------------------+-----------+\n|database|           tableName|isTemporary|\n+--------+--------------------+-----------+\n|        |weatherdistancetable|       true|\n+--------+--------------------+-----------+\n\n"}], "source": "spark.sql(\"show tables\").show()"}, {"cell_type": "markdown", "metadata": {}, "source": "Podemos hacer consultas sobre ella"}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+------------+--------+\n|WeatherDelay|Distance|\n+------------+--------+\n|        null|  374.00|\n|        null|  207.00|\n|        null|  395.00|\n|        null|  395.00|\n|        null|  395.00|\n+------------+--------+\n\n"}], "source": "flightsLejosDF = spark.sql(\"select * from weatherDistanceTable where Distance > 200 limit 5\")\nflightsLejosDF.show()"}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [{"data": {"text/plain": "DataFrame[]"}, "execution_count": 6, "metadata": {}, "output_type": "execute_result"}], "source": "spark.sql(\"CACHE TABLE weatherDistanceTable\")"}, {"cell_type": "markdown", "metadata": {}, "source": "## 4. Creamos una tabla persistente manejada, guardando como tabla el resultado de una operaci\u00f3n con el DF"}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [], "source": "flightsJFK = flightsDF.where(\"Origin = 'JFK'\").cache()\nflightsJFK.write.saveAsTable(\"flightsjfk\") # es una acci\u00f3n: se guardan f\u00edsicamente los datos en alg\u00fan sitio de HDFS"}, {"cell_type": "markdown", "metadata": {}, "source": "Si volvemos a mostrar las tablas que existen, veremos la nueva. Vemos que **no** es temporal, pero no sabemos si es manejada o es externa."}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+--------+--------------------+-----------+\n|database|           tableName|isTemporary|\n+--------+--------------------+-----------+\n| default|          flightsjfk|      false|\n|        |weatherdistancetable|       true|\n+--------+--------------------+-----------+\n\n"}], "source": "spark.sql(\"show tables\").show()"}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----------------------------+--------------------------------------------------------------+-------+\n|col_name                    |data_type                                                     |comment|\n+----------------------------+--------------------------------------------------------------+-------+\n|Month                       |string                                                        |null   |\n|DayofMonth                  |string                                                        |null   |\n|DayOfWeek                   |string                                                        |null   |\n|FlightDate                  |string                                                        |null   |\n|Origin                      |string                                                        |null   |\n|OriginCity                  |string                                                        |null   |\n|Dest                        |string                                                        |null   |\n|DestCity                    |string                                                        |null   |\n|DepTime                     |string                                                        |null   |\n|DepDelay                    |string                                                        |null   |\n|ArrTime                     |string                                                        |null   |\n|ArrDelay                    |string                                                        |null   |\n|Cancelled                   |string                                                        |null   |\n|CancellationCode            |string                                                        |null   |\n|Diverted                    |string                                                        |null   |\n|ActualElapsedTime           |string                                                        |null   |\n|AirTime                     |string                                                        |null   |\n|Distance                    |string                                                        |null   |\n|CarrierDelay                |string                                                        |null   |\n|WeatherDelay                |string                                                        |null   |\n|NASDelay                    |string                                                        |null   |\n|SecurityDelay               |string                                                        |null   |\n|LateAircraftDelay           |string                                                        |null   |\n|                            |                                                              |       |\n|# Detailed Table Information|                                                              |       |\n|Database                    |default                                                       |       |\n|Table                       |flightsjfk                                                    |       |\n|Owner                       |root                                                          |       |\n|Created Time                |Tue Mar 08 21:50:53 UTC 2022                                  |       |\n|Last Access                 |Thu Jan 01 00:00:00 UTC 1970                                  |       |\n|Created By                  |Spark 2.4.8                                                   |       |\n|Type                        |MANAGED                                                       |       |\n|Provider                    |parquet                                                       |       |\n|Table Properties            |[transient_lastDdlTime=1646776253]                            |       |\n|Statistics                  |502236 bytes                                                  |       |\n|Location                    |hdfs://ucmcluster-m/user/hive/warehouse/flightsjfk            |       |\n|Serde Library               |org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe   |       |\n|InputFormat                 |org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat |       |\n|OutputFormat                |org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat|       |\n|Storage Properties          |[serialization.format=1]                                      |       |\n+----------------------------+--------------------------------------------------------------+-------+\n\n"}], "source": "spark.sql(\"describe formatted flightsjfk\").show(50, truncate = False)"}, {"cell_type": "markdown", "metadata": {}, "source": "### Como vemos, el campo Location indica la carpeta donde se est\u00e1n guardando las tablas *manejadas*, que es la carpeta /user/warehouse/<nombretabla> de HDFS"}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+--------+---------+-------+\n|col_name|data_type|comment|\n+--------+---------+-------+\n|    Type|  MANAGED|       |\n+--------+---------+-------+\n\n"}], "source": "spark.sql(\"describe formatted flightsjfk\").where(\"col_name = 'Type'\").show()"}, {"cell_type": "markdown", "metadata": {}, "source": "## 5. Guardamos flightsDF como fichero parquet en HDFS (nada de tablas a\u00fan)"}, {"cell_type": "markdown", "metadata": {}, "source": "Lo vamos a guardar particionado por la columna Origin. Como este DF s\u00f3lo tiene dos aeropuertos distintos porque hemos retenido solamente los vuelos que salen de SFO o de LAX, Spark crear\u00e1 dos subcarpetas. Dentro de cada subcarpeta habr\u00e1 tantos ficheros como particiones tiene el DF, que actualmente son 3. Se puede comprobar con `flightsSFO.rdd.getNumPartitions()`"}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [], "source": "flightsSFO = flightsDF.where(\"Origin = 'SFO' or Origin = 'LAX'\")\\\n                      .select(\"FlightDate\", \"Origin\", \"Dest\", \"Distance\")\nflightsSFO.write.mode(\"overwrite\").partitionBy(\"Origin\").parquet(\"/flightsSFO.parquet\")"}, {"cell_type": "code", "execution_count": 12, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Found 3 items\ndrwxr-xr-x   - root hadoop          0 2022-03-08 21:51 /flightsSFO.parquet/Origin=LAX\ndrwxr-xr-x   - root hadoop          0 2022-03-08 21:51 /flightsSFO.parquet/Origin=SFO\n-rw-r--r--   2 root hadoop          0 2022-03-08 21:51 /flightsSFO.parquet/_SUCCESS\n"}], "source": "!hdfs dfs -ls /flightsSFO.parquet"}, {"cell_type": "code", "execution_count": 13, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Found 3 items\n-rw-r--r--   2 root hadoop      25790 2022-03-08 21:50 /flightsSFO.parquet/Origin=LAX/part-00000-392c83ec-1db3-4192-9c4c-9b008aa7cbc7.c000.snappy.parquet\n-rw-r--r--   2 root hadoop      27613 2022-03-08 21:51 /flightsSFO.parquet/Origin=LAX/part-00001-392c83ec-1db3-4192-9c4c-9b008aa7cbc7.c000.snappy.parquet\n-rw-r--r--   2 root hadoop      24851 2022-03-08 21:51 /flightsSFO.parquet/Origin=LAX/part-00002-392c83ec-1db3-4192-9c4c-9b008aa7cbc7.c000.snappy.parquet\n"}], "source": "!hdfs dfs -ls /flightsSFO.parquet/Origin=LAX"}, {"cell_type": "code", "execution_count": 14, "metadata": {}, "outputs": [{"data": {"text/plain": "3"}, "execution_count": 14, "metadata": {}, "output_type": "execute_result"}], "source": "flightsSFO.rdd.getNumPartitions()"}, {"cell_type": "markdown", "metadata": {}, "source": "## 6. Ahora vamos a crear una tabla EXTERNA a partir del fichero existente /flightsSFO.parquet (que en realidad es una carpeta)"}, {"cell_type": "markdown", "metadata": {}, "source": "### Hay varias maneras de crear una tabla externa. Aqu\u00ed vamos a crear una tabla externa a partir de un fichero (carpeta) que ya existe. M\u00e1s adelante veremos otra manera."}, {"cell_type": "markdown", "metadata": {}, "source": "Ojo: tenemos que especificar bien el esquema de la tabla que estamos creando a partir del fichero. El DF que hemos guardado en Parquet ten\u00eda 4 columnas, todas de tipo string"}, {"cell_type": "code", "execution_count": 25, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- FlightDate: string (nullable = true)\n |-- Origin: string (nullable = true)\n |-- Dest: string (nullable = true)\n |-- Distance: string (nullable = true)\n\n"}], "source": "flightsSFO.printSchema()"}, {"cell_type": "markdown", "metadata": {}, "source": "### 6.1. Primera manera: especificar `external table` e indicar el esquema de la nueva tabla"}, {"cell_type": "code", "execution_count": 15, "metadata": {}, "outputs": [{"data": {"text/plain": "DataFrame[]"}, "execution_count": 15, "metadata": {}, "output_type": "execute_result"}], "source": "spark.sql(\"create external table flightsSFO(FlightDate string, Origin string, Dest string, Distance string)\\\n          stored as parquet location '/flightsSFO.parquet'\")"}, {"cell_type": "markdown", "metadata": {}, "source": "### 6.2. Segunda manera (RECOMENDADA): no indicar el esquema pero indicar `using parquet location <ruta>` y la tabla autom\u00e1ticamente se crear\u00e1 con el esquema de ese fichero Parquet"}, {"cell_type": "code", "execution_count": 16, "metadata": {}, "outputs": [{"data": {"text/plain": "DataFrame[]"}, "execution_count": 16, "metadata": {}, "output_type": "execute_result"}], "source": "spark.sql(\"create table flightssfo2 using parquet location '/flightsSFO.parquet'\")"}, {"cell_type": "markdown", "metadata": {}, "source": "### Comprobamos que ahora tenemos dos tablas m\u00e1s, persistentes y con id\u00e9ntico esquema, llamadas flightSFO y flightssfo2"}, {"cell_type": "code", "execution_count": 17, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+--------+--------------------+-----------+\n|database|tableName           |isTemporary|\n+--------+--------------------+-----------+\n|default |flightsjfk          |false      |\n|default |flightssfo          |false      |\n|default |flightssfo2         |false      |\n|        |weatherdistancetable|true       |\n+--------+--------------------+-----------+\n\n"}], "source": "spark.sql(\"show tables\").show(truncate = False)"}, {"cell_type": "markdown", "metadata": {}, "source": "### \u00bfLa tabla flightsSFO es externa, o es manejada?"}, {"cell_type": "code", "execution_count": 19, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----------------------------+--------------------------------------------------------------+-------+\n|col_name                    |data_type                                                     |comment|\n+----------------------------+--------------------------------------------------------------+-------+\n|FlightDate                  |string                                                        |null   |\n|Origin                      |string                                                        |null   |\n|Dest                        |string                                                        |null   |\n|Distance                    |string                                                        |null   |\n|                            |                                                              |       |\n|# Detailed Table Information|                                                              |       |\n|Database                    |default                                                       |       |\n|Table                       |flightssfo                                                    |       |\n|Owner                       |root                                                          |       |\n|Created Time                |Tue Mar 08 21:53:20 UTC 2022                                  |       |\n|Last Access                 |Thu Jan 01 00:00:00 UTC 1970                                  |       |\n|Created By                  |Spark 2.4.8                                                   |       |\n|Type                        |EXTERNAL                                                      |       |\n|Provider                    |hive                                                          |       |\n|Table Properties            |[transient_lastDdlTime=1646776400]                            |       |\n|Statistics                  |145502 bytes                                                  |       |\n|Location                    |hdfs://ucmcluster-m/flightsSFO.parquet                        |       |\n|Serde Library               |org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe   |       |\n|InputFormat                 |org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat |       |\n|OutputFormat                |org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat|       |\n|Storage Properties          |[serialization.format=1]                                      |       |\n|Partition Provider          |Catalog                                                       |       |\n+----------------------------+--------------------------------------------------------------+-------+\n\n"}], "source": "spark.sql(\"describe formatted flightssfo\").show(50, truncate = False)"}, {"cell_type": "markdown", "metadata": {}, "source": "Interesante: en el primer caso la tabla se cre\u00f3 con provider Hive como vemos m\u00e1s arriba (fue la que hab\u00edamos indicado el esquema) y en el segundo, se cre\u00f3 con provider Parquet y entonces nos da info de particionado - a pesar de que ambas tablas est\u00e1n particionadas porque tienen el mismo origen de datos: la carpeta en la que figuran las dos subcarpetas"}, {"cell_type": "code", "execution_count": 20, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----------------------------+--------------------------------------------------------------+-------+\n|col_name                    |data_type                                                     |comment|\n+----------------------------+--------------------------------------------------------------+-------+\n|FlightDate                  |string                                                        |null   |\n|Dest                        |string                                                        |null   |\n|Distance                    |string                                                        |null   |\n|Origin                      |string                                                        |null   |\n|# Partition Information     |                                                              |       |\n|# col_name                  |data_type                                                     |comment|\n|Origin                      |string                                                        |null   |\n|                            |                                                              |       |\n|# Detailed Table Information|                                                              |       |\n|Database                    |default                                                       |       |\n|Table                       |flightssfo2                                                   |       |\n|Owner                       |root                                                          |       |\n|Created Time                |Tue Mar 08 21:53:21 UTC 2022                                  |       |\n|Last Access                 |Thu Jan 01 00:00:00 UTC 1970                                  |       |\n|Created By                  |Spark 2.4.8                                                   |       |\n|Type                        |EXTERNAL                                                      |       |\n|Provider                    |parquet                                                       |       |\n|Table Properties            |[transient_lastDdlTime=1646776401]                            |       |\n|Location                    |hdfs://ucmcluster-m/flightsSFO.parquet                        |       |\n|Serde Library               |org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe   |       |\n|InputFormat                 |org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat |       |\n|OutputFormat                |org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat|       |\n|Storage Properties          |[serialization.format=1]                                      |       |\n|Partition Provider          |Catalog                                                       |       |\n+----------------------------+--------------------------------------------------------------+-------+\n\n"}], "source": "spark.sql(\"describe formatted flightssfo2\").show(50, truncate = False)"}, {"cell_type": "markdown", "metadata": {}, "source": "### Ambas tablas son externas y la ubicaci\u00f3n f\u00edsica de los datos asociados a ella es la carpeta /flightsSFO.parquet tal como hab\u00edamos indicado al crear ambas"}, {"cell_type": "code", "execution_count": 16, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+--------+---------+-------+\n|col_name|data_type|comment|\n+--------+---------+-------+\n|    Type| EXTERNAL|       |\n+--------+---------+-------+\n\n"}], "source": "spark.sql(\"describe formatted flightssfo\").where(\"col_name = 'Type'\").show()"}, {"cell_type": "markdown", "metadata": {}, "source": "## 6.3 Tercera manera de crear una tabla externa: en una misma operaci\u00f3n guardamos nuevo el DF en otra ubicaci\u00f3n y creamos al mismo tiempo una tabla externa sobre dichos datos\n\n### El detalle para que la tabla sea creada como EXTERNA es indicar `.option(\"path\", \"...\")` antes de `saveAsTable`. Vamos a guardar el DF en /user/flights de HDFS"}, {"cell_type": "code", "execution_count": 24, "metadata": {}, "outputs": [], "source": "flightsJFK.select(\"FlightDate\", \"Origin\", \"Dest\", \"Distance\")\\\n          .write.option(\"path\", \"/user/flights\").saveAsTable(\"flightsjfk_externa\")"}, {"cell_type": "code", "execution_count": 25, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+--------+--------------------+-----------+\n|database|           tableName|isTemporary|\n+--------+--------------------+-----------+\n| default|          flightsjfk|      false|\n| default|  flightsjfk_externa|      false|\n| default|          flightssfo|      false|\n| default|         flightssfo2|      false|\n|        |weatherdistancetable|       true|\n+--------+--------------------+-----------+\n\n"}], "source": "spark.sql(\"show tables\").show()"}, {"cell_type": "markdown", "metadata": {}, "source": "Comprobamos que efectivamente se ha creado como tabla externa:"}, {"cell_type": "code", "execution_count": 27, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----------------------------+--------------------------------------------------------------+-------+\n|col_name                    |data_type                                                     |comment|\n+----------------------------+--------------------------------------------------------------+-------+\n|FlightDate                  |string                                                        |null   |\n|Origin                      |string                                                        |null   |\n|Dest                        |string                                                        |null   |\n|Distance                    |string                                                        |null   |\n|                            |                                                              |       |\n|# Detailed Table Information|                                                              |       |\n|Database                    |default                                                       |       |\n|Table                       |flightsjfk_externa                                            |       |\n|Owner                       |root                                                          |       |\n|Created Time                |Tue Mar 08 22:10:40 UTC 2022                                  |       |\n|Last Access                 |Thu Jan 01 00:00:00 UTC 1970                                  |       |\n|Created By                  |Spark 2.4.8                                                   |       |\n|Type                        |EXTERNAL                                                      |       |\n|Provider                    |parquet                                                       |       |\n|Table Properties            |[transient_lastDdlTime=1646777440]                            |       |\n|Statistics                  |40861 bytes                                                   |       |\n|Location                    |hdfs://ucmcluster-m/user/flights                              |       |\n|Serde Library               |org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe   |       |\n|InputFormat                 |org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat |       |\n|OutputFormat                |org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat|       |\n|Storage Properties          |[serialization.format=1]                                      |       |\n+----------------------------+--------------------------------------------------------------+-------+\n\n"}], "source": "spark.sql(\"describe formatted flightsjfk_externa\").show(50, truncate = False)"}, {"cell_type": "markdown", "metadata": {}, "source": "## Vamos a comprobar el efecto de guardar un DF en una carpeta ya existente, sobreescribiendo solamente las particiones de los valores que est\u00e9n presentes en los datos que ahora estamos guardando\n\nComo flightsJFK s\u00f3lo tiene Origin el aeropuerto JFK, podr\u00edamos guardarlo en la misma ubicaci\u00f3n que flightsSFO y pedirle `.mode(\"overwrite\").partitionBy(\"Origin\")` y esa operaci\u00f3n NO borrar\u00e1 lo que ya hab\u00eda sino que solamente reemplazar\u00e1 las particiones que ya existieran. Como la \u00fanica partici\u00f3n que ahora se va a crear es la de JFK y esa no exist\u00eda, el resultado es que simplemente se a\u00f1ade una partici\u00f3n m\u00e1s (es decir una subcarpeta Origin=JFK) a la carpeta.\n\n**IMPORTANTE**: el DataFrame que vamos a guardar particionado para sobreescribir ciertas particiones debe tener exactamente EL MISMO ESQUEMA que los datos de las particiones ya existentes.\n\n**IMPORTANTE**: esta operaci\u00f3n funciona habitualmente sin ninguna configuraci\u00f3n adicional en cualquier entorno. Sin embargo en Dataproc es necesario fijar expl\u00edcitamente el par\u00e1metro de Spark llamado `\"spark.sql.sources.partitionOverwriteMethod\"` al valor `\"dynamic\"`. \n\nEl m\u00e9todo habitual para dar valor a los par\u00e1metros es llamar a `spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")`. Podemos consultar qu\u00e9 valor tiene actualmente (es STATIC) poniendo\n`spark.conf.get(\"spark.sql.sources.partitionOverwriteMode\")`. \n\nAunque en este ejemplo en realidad hemos \"a\u00f1adido\" una nueva partici\u00f3n porque los datos que vamos a guardar la segunda vez no contienen ning\u00fan origen ya existente (solamente un Origin *nuevo* que es JFK), ocurrir\u00eda exactamente lo mismo si los datos s\u00ed tuviesen alg\u00fan aeropuerto de particiones ya existentes: se reemplazar\u00eda completa la partici\u00f3n de ese aeropuerto ya existente. Si adem\u00e1s hubiese aeropuertos que todav\u00eda no exist\u00edan y no ten\u00edan subcarpeta, se crear\u00edan nuevas particiones (nuevas subcarpetas) para dichos aeropuertos, sin tocar las subcarpetas existentes."}, {"cell_type": "code", "execution_count": 31, "metadata": {}, "outputs": [], "source": "# IMPRESCINDIBLE para que s\u00f3lo se reemplace en la carpeta las particiones que estemos escribiendo (recordar poner partitionBy porque en caso contrario se reemplaza la carpeta completa!)\nspark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n\n# IMPRESCINDIBLE para que el DataFrame que vamos a guardar tenga el mismo esquema que los datos ya existentes\nflightsJFK.select(\"FlightDate\", \"Origin\", \"Dest\", \"Distance\")\\\n          .write\\\n          .mode(\"overwrite\")\\\n          .partitionBy(\"Origin\")\\\n          .parquet(\"/flightsSFO.parquet\")"}, {"cell_type": "markdown", "metadata": {}, "source": "### Comprobamos que ha a\u00f1adido una subcarpeta `Origin=JFK` y no ha borrado nada de lo que hab\u00eda en esa carpeta"}, {"cell_type": "code", "execution_count": 32, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Found 4 items\ndrwxr-xr-x   - root hadoop          0 2022-03-08 22:27 /flightsSFO.parquet/Origin=JFK\ndrwxr-xr-x   - root hadoop          0 2022-03-08 21:51 /flightsSFO.parquet/Origin=LAX\ndrwxr-xr-x   - root hadoop          0 2022-03-08 21:51 /flightsSFO.parquet/Origin=SFO\n-rw-r--r--   2 root hadoop          0 2022-03-08 22:27 /flightsSFO.parquet/_SUCCESS\n"}], "source": "!hdfs dfs -ls /flightsSFO.parquet"}, {"cell_type": "markdown", "metadata": {}, "source": "## 7. Borramos tabla externa y comprobamos que el fichero Parquet sigue ah\u00ed"}, {"cell_type": "code", "execution_count": 38, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+--------+--------------------+-----------+\n|database|           tableName|isTemporary|\n+--------+--------------------+-----------+\n| default|          flightsjfk|      false|\n| default|  flightsjfk_externa|      false|\n| default|         flightssfo2|      false|\n|        |weatherdistancetable|       true|\n+--------+--------------------+-----------+\n\n"}], "source": "spark.sql(\"drop table flightssfo\")\nspark.sql(\"show tables\").show()"}, {"cell_type": "markdown", "metadata": {}, "source": "### Comprobemos que sigue existiendo la carpeta /flightsSFO.parquet"}, {"cell_type": "code", "execution_count": 39, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Found 5 items\ndrwxr-xr-x   - root   hadoop          0 2022-03-08 22:27 /flightsSFO.parquet\ndrwxrwxrwt   - mapred hadoop          0 2022-03-08 21:45 /hadoop\ndrwxrwxrwt   - mapred hadoop          0 2022-03-08 21:45 /tmp\ndrwxrwxrwt   - hdfs   hadoop          0 2022-03-08 22:10 /user\ndrwx-wx-wx   - hive   hadoop          0 2022-03-08 21:45 /var\n"}], "source": "!hdfs dfs -ls /"}, {"cell_type": "code", "execution_count": 40, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Found 4 items\ndrwxr-xr-x   - root hadoop          0 2022-03-08 22:27 /flightsSFO.parquet/Origin=JFK\ndrwxr-xr-x   - root hadoop          0 2022-03-08 21:51 /flightsSFO.parquet/Origin=LAX\ndrwxr-xr-x   - root hadoop          0 2022-03-08 21:51 /flightsSFO.parquet/Origin=SFO\n-rw-r--r--   2 root hadoop          0 2022-03-08 22:27 /flightsSFO.parquet/_SUCCESS\n"}], "source": "!hdfs dfs -ls /flightsSFO.parquet"}, {"cell_type": "markdown", "metadata": {}, "source": "## 8. Comprobamos d\u00f3nde est\u00e1n los datos de la tabla persistente manejada que hab\u00edamos guardado con saveAsTable"}, {"cell_type": "code", "execution_count": 41, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Found 1 items\ndrwxr-xr-x   - root hadoop          0 2022-03-08 21:50 /user/hive/warehouse/flightsjfk\n"}], "source": "!hdfs dfs -ls /user/hive/warehouse"}, {"cell_type": "code", "execution_count": 42, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Found 4 items\n-rw-r--r--   2 root hadoop          0 2022-03-08 21:50 /user/hive/warehouse/flightsjfk/_SUCCESS\n-rw-r--r--   2 root hadoop     150918 2022-03-08 21:50 /user/hive/warehouse/flightsjfk/part-00000-3a083775-c2fc-43bf-afd2-20cb7c111df5-c000.snappy.parquet\n-rw-r--r--   2 root hadoop     192483 2022-03-08 21:50 /user/hive/warehouse/flightsjfk/part-00001-3a083775-c2fc-43bf-afd2-20cb7c111df5-c000.snappy.parquet\n-rw-r--r--   2 root hadoop     158835 2022-03-08 21:50 /user/hive/warehouse/flightsjfk/part-00002-3a083775-c2fc-43bf-afd2-20cb7c111df5-c000.snappy.parquet\n"}], "source": "!hdfs dfs -ls /user/hive/warehouse/flightsjfk"}, {"cell_type": "markdown", "metadata": {}, "source": "## 9. Borramos la tabla manejada y comprobamos que, al ser manejada, Spark ha borrado f\u00edsicamente esos datos al borrar la tabla"}, {"cell_type": "code", "execution_count": 43, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+--------+--------------------+-----------+\n|database|           tableName|isTemporary|\n+--------+--------------------+-----------+\n| default|  flightsjfk_externa|      false|\n| default|         flightssfo2|      false|\n|        |weatherdistancetable|       true|\n+--------+--------------------+-----------+\n\n"}], "source": "spark.sql(\"drop table flightsjfk\")\nspark.sql(\"show tables\").show()"}, {"cell_type": "code", "execution_count": 44, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "ls: `/user/hive/warehouse/flightsjfk': No such file or directory\n"}], "source": "!hdfs dfs -ls /user/hive/warehouse/flightsjfk"}, {"cell_type": "markdown", "metadata": {}, "source": "## 10. Si borramos la tabla flightsjfk_externa, que se cre\u00f3 como externa directamente con saveAsTable, el borrado de la tabla no borrar\u00e1 los datos de HDFS"}, {"cell_type": "code", "execution_count": 45, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+--------+--------------------+-----------+\n|database|           tableName|isTemporary|\n+--------+--------------------+-----------+\n| default|         flightssfo2|      false|\n|        |weatherdistancetable|       true|\n+--------+--------------------+-----------+\n\n"}], "source": "spark.sql(\"drop table flightsjfk_externa\")\nspark.sql(\"show tables\").show()"}, {"cell_type": "code", "execution_count": 46, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Found 4 items\n-rw-r--r--   2 root hadoop          0 2022-03-08 22:10 /user/flights/_SUCCESS\n-rw-r--r--   2 root hadoop      11516 2022-03-08 22:10 /user/flights/part-00000-6c1f167e-47f4-4bec-93d8-d14677885e49-c000.snappy.parquet\n-rw-r--r--   2 root hadoop      13120 2022-03-08 22:10 /user/flights/part-00001-6c1f167e-47f4-4bec-93d8-d14677885e49-c000.snappy.parquet\n-rw-r--r--   2 root hadoop      16225 2022-03-08 22:10 /user/flights/part-00002-6c1f167e-47f4-4bec-93d8-d14677885e49-c000.snappy.parquet\n"}], "source": "!hdfs dfs -ls /user/flights"}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.13"}}, "nbformat": 4, "nbformat_minor": 2}